
\newcommand{\x}{\mathbf{x}}
\renewcommand{\c}{\mathbf{h}}
\renewcommand{\H}{{H}}
\newcommand{\Htext}{{C}}
\newcommand{\paren}[1]{\left({#1}\right)}
\newcommand{\brackets}[1]{\left[{#1}\right]}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}.

Our probabilistic model assumes that samples taken from the same brain region share common underlying factors, each factor reflecting the expression profile of an isolated cell type. The model also assumes that the expression profiles of different brain regions are similar but not identical, and therefore we aim to jointly learn all cell-type profiles of all regions. We start by describing a  probabilistic mixture model for samples from a \em{single} brain region, which we extend to the \em{multi-region} case in section 2.2.

\subsection{Model of a single brain region}
% ---------------------------------
Let $\{\x_1, \ldots, \x_n\} \in \reals^d$ be a set of samples. Our model assumes that each sample $\x_i$ is obtained from a noisy mixture of $K$ cell types, each having its own {\em {hidden profile}} $\{\c_1, \ldots \c_K \}\in \reals^d$.

We model each sample $x_i$ as a noisy mixture
\begin{equation*}
 \x_i = \sum_{k=1}^K p_{ik} \c_k + \xi_i \quad\forall i \quad,
\end{equation*}
where $p_{ik}$ is the proportion of the cell type $k$ in the sample $i$, $\sum_k p_{ik} = 1, \forall i$, and $\xi_i \sim N(0,\sigma^2)$ is additive Gaussian noise independent at each sample. We further assume a Gaussian prior distribution $p_0( \c_1, \ldots, \c_K)$ over each cell type $ \c_k  \sim N(\mu_k, \Sigma) $, and over the mixture proportions $p_0(p_1, \ldots p_K)$. The parameters $\mu_k$ and $\Sigma$ can be estimated in advance from available measurements of expression in isolated cell types \cite{okaty2011cell,darmanis2015survey}.

Together, the probability of an observed sample $x_i$ is
\begin{equation*}
    P(\x_1,\ldots,\x_n | \{\c_k\}, \{p_{ik}\}) =  \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2\sigma} (x_i - \sum_{k=1}^K p_{ik} \c_k)^2 \right)
\end{equation*}
and the minus log posterior of all the data equals (up to linear constants)
\begin{equation*}
    - log P(\x_1,\ldots,\x_n, \H, P| \mu, \Sigma) \propto
    \sum_{i=1}^n \paren{ \frac{1}{\sigma} (x_i - \sum_{k=1}^K p_{ik} \c_k)^2 }
     + \sum_{k=1}^K (c_k-\mu_k)^T\Sigma^{-1}(c_k-\mu_k)
\end{equation*}

Using matrix notation, we denote by $\H$ the $K \times d$ matrix of hidden profiles per cell type whose columns are $\brackets{\c_1, \ldots, \c_K}$; we denote by $W$ the $n \times K$ matrix of proportions $p_{ik}$,
and by $X$ the matrix whose columns are the samples $\brackets{\x_1,..., \x_n}$. This yields that maximizing the log posterior is equivalent to minimizing 
\begin{equation}
    \label{eq-single}
    \begin{aligned}
        & \underset{\cal{\H},\cal{W}}{\text{min}}  
        & & \norm{X - W \H}_F^2 + \sigma \norm{ \H - M }^2_{(\Sigma^{-1})}\\
            & \text{subject to} &
            & W^r_{l,j} \geq 0 \;\; \forall j, l, \\
        & & & H^r_{j,d} \geq 0 \;\;\forall j, d \\
        & & & \sum_j W^i_{l,j} = 1 \;\; \forall l \\
        \end{aligned}
\end{equation}
where $\norm{\cdot}_F$ is the Frobenius norm, $M$ is a matrix whose columns are the expected cell-type profiles $\mu_k$ and  $\|\cdot\|_{\Sigma}$ is the norm through a PSD matrix $\Sigma$. The constraints on $\W$ are because the elements in each row of $\W$ are mixture probabilities. The constraints on $\H$ are because the elements of $\H$ are counts of expressed transcripts.

\subsection{Multiple regions}
%-------------------------------
We now turn to extend single-region demixing to handle demixing of samples collected from several brain regions. 
As far as we are aware, this is the first probabilistic model for demixing multiple sets of samples.

We assume that expression profiles of an each cell type can vary from one region to another, but is likely to remain similar. The strength of similarity between the expression profiles in two regions depends on how closely related two brain regions are. For instance, neurons in cortical regions may be very similar to each other, less similar to the hippocampal neurons and far less similar to the cerebellar neurons. 

To capture these relations our model introduces pair-wise attractive potentials between the cell-type profile in one region $\c^r_k$ to the profile of the same cell type in another region $\c^s_k$. The strength of the attraction depends on the relatedness $\phi_{r,s}$ of two brain regions, and on a global hyper parameter that controls the relative weight of edge potentials compared with node potentials. Together, this yields pairwise potential terms of the form 
$\lambda \phi_{r,s}|| \c^r_k - \c^s_k||^2$.

This soft-sharing approach can be seen as a balance between two extremes. At one extreme (no pairwise attraction, $\lambda=0$), demixing of each regions is trained independently. With current datasets, this approach usually has too few samples per regions.  At the other extreme (strong pairwise attraction, $\lambda \gg 0$), one could treat all samples as being created from a single set of underlying latent factors, as if all brain regions share a single expression profiles per cell-type. In this case, more samples are available for estimating the (fewer) latent factors, but the wrong assumes that expression is homogeneous across the brain   distorts the resulting profiles. Our soft-sharing approach bridges the two extremes by controlling how profiles of different regions share common traits. Similar soft sharing has often been used in supervised multi-task learning. 

Formally, given sets of samples from $R$ regions ${\cal{X}} = \{X^1,\ldots, X^R\}$, where $X^r = \{\x^r_1, \ldots, \x^r_{n_r} \}$, we learn $R$ sets of hidden cell-type-specific profiles ${\cal{\H}} = \{\H^1,\ldots, \H^R\}$ and their corresponding proportions ${\cal{W}} = \{ W^1,\ldots W^R\}$. Adding Gaussian potentials over cell-type profiles from a pair of regions ($\H^r$, $\H^s$) with a weight of $\lambda_{r,s}$ amounts to a quadratic term in the log posterior. This therefore yields the following optimization problem
\begin{equation}
    \label{eq-multi}
    \begin{aligned}
        & \underset{\cal{\H},\cal{W}}{\text{min}}  
        & & \sum_{r=1}^R  \| X^r - W^r \H^r \|_F^2
        + \sum_{r=1}^R  \sigma^r \| M^r - \H^r \|^2_{(\Sigma^{-1})}  
        + \sum_{r \neq s} \lambda \phi_{r , s} \| \H^r - \H^s \|_F^2  \\
        & \text{subject to} &
            & W^r_{l,j} \geq 0 \;\; \forall j, l, r\\
        & & & H^r_{j,d} \geq 0 \;\;\forall j, d, r \\
        & & & \sum_j W^r_{l,j} = 1 \;\; \forall l,r \;. 
    \end{aligned}
\end{equation}
Once again the constraints on $\W$ are since these are the mixture proportions, and the constraints on all $\H$ reflect the fact that $\H$ holds counts of transcripts.

The proposed model is interestingly related to Markov random fields (MRFs). Consider a case where the mixture proportions $\cal{W}$ are given, then the probabilistic model over $\cal{\H}$ can be viewed as an MRF, where nodes correspond to the profiles of individual cell types across all regions $\c^1_1,\ldots, \c^r_k, \ldots \c^R_K$, node potentials reflect the priors $p_0(\c_k)$ based on known expression of individual cell types at various regions, and edge potentials reflect region-to-region similarity terms $\lambda \phi_{r,s}|| \c^r_k - \c^s_k||^2$. 

The next section discusses the algorithms we use to optimize this objective.

\section{Optimization}
% =====================
We now describe algorithms for optimizing the multi-region problem of \eqref{eq-multi}. We start with reviewing existing approaches for single-region demixing and then discuss multi-region demixing. 


% Our problem is well suited for performing block coordinate descent, both on the profiles and proportion matrices and both on the regions. At each iteration, we update the set of coordinates of a single region and freeze the coordinate of the rest of the region. After we update the profile matrices $H_n$ we proceed to update its proportion matrix $W_n$. Then, we cycle through the rest of the region and repeat until we converge. 

\subsection{Single-region demixing}
The demixing problem of \eqref{eq-single} is separately convex in $\H$ and $\W$ (quadratic objective with linear constraints), but not jointly convex in both. When the prior is discarded, it becomes equivalent to non-negative matrix factorization (NMF) \cite{leenmfs}, which has been intensively studied. Several approaches have been proposed to minimize the NMF objective.
%
% For the convergence I think we can claim something similar to:
% http://bioinformatics.oxfordjournals.org/content/23/12/1495.full
% In the section:
% Convergence properties of sparse NMF algorithms
% http://www.sciencedirect.com/science/article/pii/S0167637799000747#
Here we tested four methods: Multiplicative updates (MU) \cite{leenmfs}, and three variants of alternative least square (ANLS) \cite{lin2007projected,kim2008activeset,kim2011fast}. 

{\bf {Multiplicative updates (MU)}}. Lee and Seung \cite{leenmfs} described an NMF algorithm based on multiplicative updates of W and \Htext. At each step, each coordinate of \Htext (and \W) is multiplied by a non-negative scalar, which guarantees that non-negativity is maintained: $H_{qj} \leftarrow H_{qj} \frac{(W^TX)_{qj}}{((W^TW)H)_{qj}} \quad
\forall 1\leq q \leq k, 1\leq j \leq n$ and $W_{iq} \leftarrow W_{iq} \frac{(XH^T)_{iq}}{(W(HH^T))_{iq}} \quad \forall 1\leq i \leq m, 1\leq q \leq k$.
This procedure is guaranteed to converge \cite{leenmfs,lin2007convergence}. 
% They showed the objective after these updates is %non-increasing. Although in practice the %muliplcative-update method works well, it was suggest %by Gonzalez and Yin Zhang %\cite{gonzalez2005accelerating}  that the %non-increasing updates may not converge to a %stationary point within realistic time.

{\bf{Alternating nonnegative least squares} (ANLS)} A second set of approaches to minimize the NMF objective is based on the fact that the optimization problem in \eqref{eq-single} is convex in each of the variables \W (or \Htext), hence one can iterate between efficiently updating $\W$ and updating $\H$. In ANLS, the two matrices are  repeatedly updated using $ W^{t+1} = \argmin_{W \geq 0 } f(W^t, H^t)$ and $H^{t+1} = \argmin_{H \geq 0 } f(W^{t+1},H^t)$. Several different method of solving non negative least squares (NNLS) have been proposed some especially tuned for NMF. \citet{lin2007projected} proposed projected gradient decent with step that is chosen using the Armijo rule. 
Kim and park \cite{kim2008activeset,kim2011fast} use an active set method to solve NNLS. They swap variables between an active set and a passive set to find the optimal sets. The intuition behind this is that if the passive set (the strictly positive variables) was known in advanced then an NNLS problem is a simple unconstrained least squares. Another variant of this concept is done by the "the block pivoting method" which allows to change multiple variable between groups per iteration (as appose to a single variable using the active set method) \cite{kim2011fast}. {\bf{THE ANLS SECTION SHOULD BE MUCH SHORTER}}

In \eqref{eq-single} the rows of $\W$ are constrained to sum to one. This constraint is often relaxed during optimization. In practice, it is often replaced with a simple heuristic of normalizing each row of W after the optimization is completed. 

The above discussion focused on the first term in \eqref{eq-single}, omitting the prior term. Using the fact that we alternate between $\W$ and $\H$ the priors can be elegantly added by a simple transformation. Each time before we solve the minimization problem for H we perform:
$ X_{priors} = \Big[ X ;\Sigma^{-\frac{1}{2}} \Big]  $  ,  $ \W_{priors} = \Big[  \W ; \Sigma^{-\frac{1}{2}} M \Big]  $. We then find the minimum for $H$ using $X_{priors} $ and $W_{priors} $. 



\subsection{Multi-region demixing}
% ----------------------------------
The multi-region demixing model of \eqref{eq-multi} defines a joint optimization problem over all cell-type profiles $\cal{\H}$ and their proportions across all regions $\cal{\W}$.

Given values for the hyper parameters $\lambda$, $\sigma^r$ and $\phi_{r,s}$, 
we optimize \eqref{eq-multi} by extending the alternating nonnegative least square (ANLS) approach, this time solving separately for ${\H^r}$ and ${\W^r}$ of each region given all the other regions. %This approach has similar convergence guarantees as the original ANLS approach.  {\bf{ LIOR CAN YOU SPECIFY? AND ADD A REF}}

We initialize $\W^r$ with values in the range [0,1] and initialize $\H^r$ by random data samples with multiplicative noise. In practice, we found that it helped to start optimization by solving for each region separately, alternating over $\H$ and $\W$ for that region. We believe that this helped the profiles to more easily reconstruct their own unique profile. This 'warm-start' was then used to initialize the global ANLS optimization. 
% We have also experimented with a few method to enforce the constrains on the proportion matrix. while some s%%Then prior to starting the loop, we warm initialize the profiles by performing a single step of block coordinate decent for each ($H^r,W^r$).

The values of the hyper parameters $\lambda$ and $\sigma^r$ can be found using cross validation. However, searching over all pairwise weights $\phi_{r,s}$ maybe prohibitive. We therefore used prior knowledge to determine the values of $\phi_{r,s}$. The strength of these hyper parameters reflects our prior belief on how strongly cell-type specific expression should be similar in two given regions, like the the expected similarity of a cortical to a hippocampal neuron.
To determine the values of $\phi_{r,s}$, we used a brain region hierarchy developed by the Allen institute of brain research (\figref{fig:bro}) to compute the tree-depth of the closest common ancestor of two brain regions $r$ and $s$. We used this depth as an estimate of $\log(\phi_{r,s})$. For example, the cerebellum and the occipital lobe  in this hierarchy are joined only at the root so we set their $\phi_{cerebellum, occipital} = \exp(1)$. 
%Specifically, at stage $t+1$ we update the profile matrix of %region r$ \H_r^{t+1} $ using the related profiles as priors.
%For the regularize parameters we use a base $\lambda$ times a distance factor between pairs of regions. 

Algorithm (1) describes our training procedure.

\begin{algorithm}[tbh]
   \caption{Multi-region demixing}
   \label{alg:multimix}
   \begin{algorithmic}[1]
   \STATE {\bfseries input:} training data $X$, number of steps, $\lambda$, $\{\sigma^r\}$, $\{\phi_{r,s}\}$, $\{\mu^r_k\}$, $\Sigma$
   \STATE {\bfseries initialize:} for every region $r$
   \STATE \quad Initialize $\W^r$ and $\H^r$ with random values.
   %\REPEAT
   \STATE {\bfseries warm-start:} for every region $r$ 
   \STATE \quad Find optimal $\W^r$ and $\H^r$ for the single-region objective \eqref{eq-single}
%   \STATE \quad $H^r = \argmin_{H \geq 0 } \| X^r - W^rH\|^2_F +  \sigma^r \| M^r - \H \|^2_{(\Sigma^{-1})} $
 %  \STATE \quad $ W^r = \argmin_{W \geq 0} \| X^r - WH^r\|^2_F $
   \REPEAT
   \STATE Randomly permute the order of the regions.
   \STATE Iterate over the permuted regions list and select a region $r$.
   \STATE \quad Find optimal $\W^r$ , $\H^r$ for the multi-region objective \eqref{eq-multi} use other regions as priors.
   \STATE \quad $H^r = \argmin_{H \geq 0 } \| X^r - W^rH\|^2_F + \sigma^r \| M^r - \H \|^2_{(\Sigma^{-1})} + \sum_{r \neq s} \lambda \phi_{r , s} \| \H - \H^s \|_F^2  $
   \STATE \quad $ W^r = \argmin_{W \geq 0} \| X^r - WH^r\|^2_F $
   \UNTIL{stopping condition}
\end{algorithmic}
\end{algorithm}


