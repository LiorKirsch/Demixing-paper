

With the goal of comparing optimization methods and finding the regime of hyper parameters where multi-region demixing is beneficial, we start with a series of experiment on synthetic data where the latent factors are known and used as a ground truth. We first describe how the dataset was created and then characterize how performance depends on the number of samples and the number of latent factors (cell types).


\subsubsection*{Creating the mixture dataset}
We created controlled mixtures from a set of real transcriptome measurements, collected from isolated cells



We gathered profiles for the 3 population of cell types in the brain: neurons, astrocytes and oligodendrocytes. Each profile is represented by 14580 genes. The cell type profiles were extracted from 7 brain regions:  cortex layer 5A, cortex layer 5B, cortex layer 6, striatum, cerebellum, brainstem and the spinal cord \cite{okaty2011cell}. For each region, we mixed the profiles using ratio taken from the literature \cite{Herculano2014}. cortical regions ( 0.7; 0.1;0.2) , cerebellum (0.5; 0.15; 0.35) , other (0.65;  0.1;0.25)  (neuron, astrocytes, oligodendrocytes).

We have added variability to the proportions of the samples by multiplying them by random value [0,1] and scaling back so that they sum to 1.
Finally, we introduced noise for each sample by multiplying it with by a normal random variable with unit mean. We insure that the normally distributed number is positive.

We evaluated the reconstructed profiles by computing their spearman correlation with the true profiles. We then, matched the best true profiles to the reconstructed profiles.

Each time we sampled n noisy samples and reconstructed the profiles using these samples. The error bars in figure  represented the sem over 30 repetitions.  

\subsubsection*{analyzing number of samples}
We notice that in the regime of limited samples there is sweet spot where we benefit from using regularization over the naive cases. As we increase the number of samples the benefit that we gain by using our method diminishes Figure \label{fig:controlled_exp}(a). With enough samples we solving the individual problems performs just as well. 



\begin{figure}[!hbt]
   (a) \hspace{120pt}(b) \hspace{120pt}(c) \hspace{120pt}
   \centering
     \includegraphics[width=0.32\textwidth]{methods}
     \includegraphics[width=0.32\textwidth]{lambda_samples}
     \includegraphics[width=0.32\textwidth]{num_types}
    \caption{The effect of $\lambda$ on reconstruction}, 
    (a)  With a single region, all optimization method perform better as we introduced more samples. With a limited number of samples the active set and the block pivoting approaches performed the best. (b) Using multiple regions, the regularized connection between the region profiles help to reconstruct the original profiles. This is especially noticeable when only few samples are available for each region. (c) ????
    \label{fig:controlled_exp}
\end{figure}




\subsubsection*{analyzing number of cell types}





